"""
This component of the project adresses the disparities in the dataset size between the symmetricity and monotonicity dataset 
by augmenting the datasets to address limitations in scope and volume of our data

our prinicipal applications are: 

(1) - Augmenting the monotonicity training dataset to roughly 10,000 total pairs
        5000 negated sentence pairs : Original dataset (~ 1000)
        5000 positives sentence pairs : Original Dataset (~ 1500)

    The template structure is of format :
    {"sentence": this is a sample sentence, strong: "sentence", weak: ["complex sentence", "simple sentence"]}
    
    the outputted pairs or of the form:
    
    premise: strong sentence, hypothesis: weak sentence 1, label: entailment
    premise: strong sentence, hypothesis: weak sentence 2, label: entailment
    premise: weak sentence 1, hypothesis: strong sentence, label: non-entailment
    premise: weak sentence 2, hypothesis: strong sentence, label: non-entailment

(2) - Increasing diversity of relationships in symmetricity (?)
"""



## must conda/pip install openai,dotenvs
from openai import OpenAI
import pandas as pd
from dotenv import load_dotenv
import os

import json

load_dotenv()
#this is set as an environment variable, since the repo is public
#I should have plenty of credit on my account so long as we use a mini model in batch mode, ping me for api key
openai_api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI()



def generate_monotonicity_templates(sample_data:str, num_new_samples:int,request_id:str):
    """
    Generates a batch request with a fixed prompt determined by the specific scope of (1)
    
    parameters:
        sample_data (str): sample output for the model, hand generated using original data as reference
        num_new_samples (int): number of sentence templates generated by the prompt. Does not directly correspond to the number of NLI pairs
    Returns:
        batch_file_id (str): id of the batch request sent to openai
    """

    prompt = f"""
    You are going to produce a dataset of templates for a Monotonicity Natural Language Inference Task formatted as a jsonl file. Model your templates on the provided samples.

    Samples:

    {sample_data}
    

    generate {num_new_samples} diverse samples,  disjoint samples. Vary the sentences in length and topic matter
    """
    query_response = create_batch_request(prompt,request_id) 
    return query_response

    
def create_batch_request(prompt, request_id): 
    """
    Somewhat generic function to generate an openai batch request assuming that the batch includes a single request
    """
    
    request = {
        "custom_id": request_id,
        "method": "POST",
        "url": "/v1/responses",
        "body": 
            {
                "model": "gpt-5-mini",
                "input": [{"role":"user", "content": prompt}], 
            }
    }
    
    with open("temp.jsonl","w") as json_file:
        json.dump(request,json_file)
    
    batch_input_file = client.files.create(
        file = open("temp.jsonl","rb"),
        purpose= "batch"
    )

    batch_id = batch_input_file.id

    batch_info = client.batches.create( 
        input_file_id = batch_id,
        endpoint= "/v1/responses",
        completion_window = "24h",
    )

    os.remove("temp.jsonl")

    return {"request_id": request_id, "batch_file_id": batch_info.id}


def fetch_batch_results(batch_id:str):
    batch = dict(client.batches.retrieve(batch_id))
    if batch['status'] == 'completed':
        response = client.files.content(batch['output_file_id']).text
        print(response)
    else:
        print("current status: ",batch['status'])
        print(batch)
    

def main():

    negative_sample_data = "{'sentence': 'There is a man not wearing a hat staring at people on a subway.', 'strong': 'hat', 'weak': ['sombrero', 'sunhat', 'fedora']} \n{'sentence': 'The three children are not holding plants.', 'strong': 'plants', 'weak':['daisies', 'flowers', 'Lillies', 'ferns', 'houseplants]}"

    test_request = generate_monotonicity_templates(sample_data=negative_sample_data,num_new_samples=5,request_id="small-test-2")
    with open("request_log.jsonl","a") as request_logs:
        request_logs.write("\n")
        json.dump(test_request,request_logs)
    
    fetch_batch_results(test_request['batch_file_id'])


#main()

## for right now: just check the request log file for your batch of interest
#will be made more robust in the near future
fetch_batch_results("batch_690bc14413c88190a8f96dd889ccda60")