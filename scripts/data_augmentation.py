"""
This component of the project adresses the disparities in the dataset size between the symmetricity and monotonicity dataset 
by augmenting the datasets to address limitations in scope and volume of our data

our prinicipal applications are: 

(1) - Augmenting the monotonicity training dataset to roughly 10,000 total pairs
        5000 negated sentence pairs : Original dataset (~ 1000)
        5000 positives sentence pairs : Original Dataset (~ 1500)

    The template structure is of format :
    {"sentence": this is a sample sentence, strong: "sentence", weak: ["complex sentence", "simple sentence"]}
    
    the outputted pairs or of the form:
    
    premise: strong sentence, hypothesis: weak sentence 1, label: entailment
    premise: strong sentence, hypothesis: weak sentence 2, label: entailment
    premise: weak sentence 1, hypothesis: strong sentence, label: non-entailment
    premise: weak sentence 2, hypothesis: strong sentence, label: non-entailment

(2) - Increasing diversity of relationships in symmetricity (?)
"""



## must conda/pip install openai,dotenvs
from openai import OpenAI
import pandas as pd
from dotenv import load_dotenv
import os

import json

load_dotenv()
#this is set as an environment variable, since the repo is public
#I should have plenty of credit on my account so long as we use a mini model in batch mode, ping me for api key
openai_api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI()
request_log_fpath = "request_log.json"



def generate_monotonicity_prompt(sample_data:str, num_new_samples:int,request_id:str):
    """
    Generates a batch request with a fixed prompt determined by the specific scope of (1)
    
    parameters:
        sample_data (str): sample output for the model, hand generated using original data as reference
        num_new_samples (int): number of sentence templates generated by the prompt. Does not directly correspond to the number of NLI pairs
    Returns:
        batch_file_id (str): id of the batch request sent to openai
    """

    prompt = f"""
    You are going to produce a dataset of templates for a Monotonicity Natural Language Inference Task formatted as a jsonl file. Model your templates on the provided samples.

    Samples:

    {sample_data}
    

    generate {num_new_samples} diverse, disjoint samples. Vary the sentences in length and topic matter. Avoid Repetition, ensure the sentences are coherent.
    """
    create_batch_request(prompt,request_id) 

    
def create_batch_request(prompt, request_id): 
    """
    Somewhat generic function to generate an openai batch request assuming that the batch includes a single request
    """
    
    request = {
        "custom_id": request_id,
        "method": "POST",
        "url": "/v1/responses",
        "body": 
            {
                "model": "gpt-5",
                "input": [{"role":"user", "content": prompt}], 
            }
    }
    
    with open("temp.jsonl","w") as json_file:
        json.dump(request,json_file)
    
    batch_input_file = client.files.create(
        file = open("temp.jsonl","rb"),
        purpose= "batch"
    )

    batch_id = batch_input_file.id

    batch_info = client.batches.create( 
        input_file_id = batch_id,
        endpoint= "/v1/responses",
        completion_window = "24h",
    )

    os.remove("temp.jsonl")

    # log the batch request
    with open(request_log_fpath,"r") as file:
        if not os.path.getsize(request_log_fpath) == 0:
            request_logs = dict(json.load(file))
        else:
            request_logs = {}

    request_logs.update({request_id: batch_info.id})
    with open(request_log_fpath,"w") as file:
        json.dump(request_logs,file)


# ------------------------dataset generation after prompt has been sent to API -----------------------------------------

def generate_monotonicity_dataset(request_id:str,output_fpath:str,column_labels:str):
    with open("request_log.json","r") as file:
        request_logs = dict(json.load(file))
    
    templates = fetch_batch_results(request_logs[request_id])
    if templates is not None:
        generate_monotonicity_tsv_file(templates=templates,output_fpath=output_fpath,column_labels=column_labels)


def fetch_batch_results(batch_id:str):
    """
    Use with caution, assumes output form of query is a line separated list of dictionaries (jsonlike)
    """
    def safe_dict_load(item:str):
        try:
            return json.loads(item)
        except:
            return None


    batch = dict(client.batches.retrieve(batch_id))
    dicts = None
    if batch['status'] == 'completed':
        if batch['output_file_id'] is not None: 
            response = json.loads(client.files.content(batch['output_file_id']).text)
            string_response = response['response']['body']['output'][1]['content'][0]['text']
            response = string_response.split("\n")
            #print(response)

            dicts = [safe_dict_load(item) for item in response if safe_dict_load(item) is not None]
            print("length: ",len(dicts))
        else:
            print("No output file generated")
    else:
        print("Not completed! currently: ",batch['status'])
    
    return dicts
        

def generate_monotonicity_tsv_file(templates:list[dict],output_fpath:str,column_labels:str):
    rows = []
    idx = 1
    for template in templates:
        for weak in template['weak']:
            strong_sentence = template['sentence'].lower()
            weak_sentence = strong_sentence.replace(template['strong'],weak)
            entailment = [idx,strong_sentence,weak_sentence,'entailment']
            idx += 1
            non_entailment = [idx,weak_sentence,strong_sentence,weak,template['strong'],'non-entailment']
            idx += 1
            rows += [entailment,non_entailment]
    
    df = pd.DataFrame(rows,columns=column_labels)
    df.to_csv(output_fpath, sep="\t",index=False)



def synthetic_verification(synth_fpath:str,train_fpath:str,test_fpath):
    """
    Programmatic verification of the validity of synthetic data:
    (1) No identical data between synth, train, and test data
    (2) random sampling of synthetic data, premise and hypothesis differ in only a single word
    (3) strong/weak pairs from random sample are extracted and manually verified ~ 200
    """




                

def main():
    negative_sample_data = """
    {'sentence': 'There is a man not wearing a hat staring at people on a subway.', 'strong': 'hat', 'weak': ['sombrero', 'sunhat', 'fedora']} 
    {'sentence': 'The three children are not holding plants.', 'strong': 'plants', 'weak':['daisies', 'flowers', 'Lillies', 'ferns', 'houseplants']}
    {'sentence': 'Some dishes do not have food'., 'strong': 'food', 'weak': ['rice','vegetables','meat']}
    """

    positive_sample_data = """
    {'sentence': 'A man is talking to someone in a car.', 'strong': 'car', 'weak': ['taxi', 'convertible', 'SUV']} 
    {'sentence': 'Two dogs play with a green ball on a wooden deck', 'strong': 'car', 'weak': ['taxi', 'convertible', 'SUV']} 
    """
    
    generate_monotonicity_prompt(sample_data=negative_sample_data, num_new_samples=1000, request_id="neg-1000-BIG")
    #generate_monotonicity_prompt(sample_data=positive_sample_data, num_new_samples=1000, request_id="pos-1000")


    monotonicity_col_labels = ['textid','text','pair','weak','strong','label']
    
    #the split of the distribution should occur at the template level
    #generate_monotonicity_dataset(request_id="neg-900-BIG",output_fpath="negative_synthetic_2.tsv",column_labels=monotonicity_col_labels)
    
    
    ### Next Step:
        # validate data programmatically w/ random sampling + parsing 

        
main()
